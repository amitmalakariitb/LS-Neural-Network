{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5343ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "import gradio as gr\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Config\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af02b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for my dataset\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path:str, tokenizer):\n",
    "        # Load the JSON data from the provided path(the dataset)\n",
    "        self.data = json.load(open(path, \"r\"))\n",
    "        \n",
    "        # Extract the text from the dialog and format it\n",
    "        self.X = []\n",
    "        for i in self.data:\n",
    "            for j in i['dialog']:\n",
    "                self.X.append(j['text'])\n",
    "\n",
    "        # Combine dialog turns to form input-output pairs\n",
    "        for idx, i in enumerate(self.X):\n",
    "            try:\n",
    "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # Limit the dataset size to the first 2000 samples\n",
    "        self.X = self.X[:2000]\n",
    "        \n",
    "        # Print the first formatted data sample\n",
    "        print(self.X[0])\n",
    "\n",
    "        # Tokenize the input data using the provided tokenizer\n",
    "        self.X_encoded = tokenizer(self.X,max_length=40,padding= True, truncation=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the tokenized input IDs and attention mask for the given index\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e5f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(chatData, model, optimizer):\n",
    "    epochs = 12\n",
    "\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        print(f\"Epoch {i}\")\n",
    "        for X, a in chatData:\n",
    "            X = X.to(device)  # Move input data to the device \n",
    "            a = a.to(device)  # Move attention mask to the device \n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss = model(X, attention_mask=a, labels=X).loss  # Compute the loss\n",
    "            print(f\"Loss = {loss.item()}\")  # Print the current loss\n",
    "\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the model's parameters using the optimizer\n",
    "\n",
    "        # Save the model's state dictionary to a file after each epoch\n",
    "        torch.save(model.state_dict(), \"model_state.pt\")\n",
    "\n",
    "        # Perform inference using the \"infer\" function on input\n",
    "        print(infer(\"hello how are you\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d99b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "    # Format the input for generation\n",
    "    inp = \"<startofstring> \" + inp + \" <bot>: \"\n",
    "\n",
    "    # Encode the formatted input using the tokenizer and move it to the device\n",
    "    input_ids = tokenizer.encode(inp, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Create an attention mask with ones for the generated input\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    # Generate  response using the model\n",
    "    output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output and remove special tokens\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07d8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the device to train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d11baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                \"bos_token\": \"<startofstring>\",\n",
    "                                \"eos_token\": \"<endofstring>\"})\n",
    "# Add the \"<bot>:\" token as a new token\n",
    "tokenizer.add_tokens([\"<bot>:\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef524bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# Resize the token embeddings of the model to match the updated tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Move the model to the specified device \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee7bd67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> I love iphone! i just bought new iphone! <bot>: Thats good for you, i'm not very into new tech <endofstring>\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ChatData by loading data\n",
    "chatData = ChatData(\"./chat_data.json\", tokenizer)\n",
    "# Create a DataLoader for the chat data with a batch size of 32\n",
    "chatData =  DataLoader(chatData, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6007c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 40.95790100097656\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 95.95818328857422\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 46.49863052368164\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 7.553525447845459\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.894950866699219\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 7.931908130645752\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 7.95673942565918\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 8.076740264892578\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 6.012801170349121\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.308113098144531\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.525579929351807\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.943215847015381\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.421879768371582\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.36928129196167\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.497930526733398\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.61627459526062\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.7168803215026855\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.573873996734619\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.549310207366943\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.049575090408325\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.537198305130005\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.3934326171875\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.856060028076172\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.399340629577637\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.1263935565948486\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.71649169921875\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.255221366882324\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.84834885597229\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.3187620639801025\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.3056416511535645\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 1.9707187414169312\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.6506407260894775\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.252518653869629\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.042572975158691\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.8761045932769775\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.8135924339294434\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.420051574707031\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.301379442214966\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.404627799987793\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.461253643035889\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.009532928466797\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.5880768299102783\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.295128345489502\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.077296733856201\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.028596878051758\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.8206379413604736\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.265831232070923\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.4015185832977295\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 5.093132495880127\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.951179027557373\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.8173441886901855\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.4028477668762207\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.4228312969207764\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.7260425090789795\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.894644260406494\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 4.593298435211182\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.4595141410827637\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.240220546722412\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 3.509251594543457\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.9124135971069336\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.8431007862091064\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.9090118408203125\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.523655891418457\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\amitm\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  8%|██████▎                                                                    | 1/12 [4:46:29<52:31:28, 17189.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>:, you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "epochs 1\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.5343971252441406\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n",
      "loss = 2.7416462898254395\n",
      "hi\n",
      "hi1\n",
      "hi2\n",
      "hi3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                    | 1/12 [4:56:01<54:16:14, 17761.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optim \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining .... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchatData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(chatData, model, optim)\u001b[0m\n\u001b[0;32m     12\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1098\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1096\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1098\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setting the model in training mode \n",
    "model.train()\n",
    "\n",
    "# Initializing an Adam optimizer for updating the model's parameters\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Print a message indicating the start of the training process\n",
    "# initially during training i have added hi messages as it was taking too long to train\n",
    "print(\"Training...\")\n",
    "\n",
    "train(chatData, model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a90104",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"infer from model : \")\n",
    "while True:\n",
    "  # Getting user input\n",
    "  inp = input()\n",
    " # Printing the generated response\n",
    "  print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575c1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from gradio.components import Textbox\n",
    "\n",
    "# Defining the configuration for the fine-tuned model to match size\n",
    "config = GPT2Config(\n",
    "    vocab_size=50261, \n",
    "    n_embd=768,  \n",
    "    n_layer=12,  \n",
    "    n_head=12,  \n",
    ")\n",
    "# Initializing and importing the trained and fine-tuned models\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  \n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  \n",
    "model = GPT2LMHeadModel(config=config)\n",
    "model.resize_token_embeddings(50261)\n",
    "model.load_state_dict(torch.load(\"model_state.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Defining a function to generate responses\n",
    "def generate_response(input_text):\n",
    "    input_text = \"<startofstring> \" + input_text + \" <bot>: \"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Creating a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=Textbox(),\n",
    "    outputs=Textbox(),\n",
    "#     layout=\"vertical\",\n",
    "    title=\"Chatbot Demo\",\n",
    "    description=\"Type a message to chat with the bot.\",\n",
    ")\n",
    "\n",
    "# Launching the Gradio interface\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
